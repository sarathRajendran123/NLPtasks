{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b81001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Uh huh and then uh what time did you get up this morning?\"\n",
    "    \"Well I guess you could say that uh huh.\"\n",
    "    \"I don't know uh huh.\"\n",
    "    \"An extension cord is something you plug into the wall.\"\n",
    "    \"It's uh it's a long cord that has a plug on one end and then it has three outlets on the other end.\"\n",
    "    \"You can plug like a lamp or a radio or something like that into it.\"\n",
    "    \"So you can have more things plugged in than you normally could have.\"\n",
    "    \"Right.\"\n",
    "    \"And then uh there's also like a surge protector which is kind of like an extension cord but it also protects your appliances from uh power surges.\"\n",
    "    \"So if there's a like a lightning strike or something like that it won't mess up your stuff.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fd595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 81\n",
      "['Uh', 'huh', 'and', 'then', 'uh', 'what', 'time', 'did', 'you', 'get']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming your sentences are stored in a list called 'sentences'\n",
    "unique_words = []\n",
    "for sentence in sentences:\n",
    "  unique_words.extend(word_tokenize(sentence))\n",
    "\n",
    "word_counts = Counter(unique_words)\n",
    "unique_words = list(word_counts.keys())\n",
    "\n",
    "print(\"Total unique words:\", len(unique_words))\n",
    "print(unique_words[:10])  # Print the first 10 unique words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af1eb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading cmudict: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\win10/nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict.zip/cmudict/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\win10/nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcmudict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the CMU Pronouncing Dictionary\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m pronouncing_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcmudict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_to_arpabet\u001b[39m(word):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    Transcribes a word to ARPAbet using the CMU Pronouncing Dictionary.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    If the word is not found in the dictionary, return None.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\win10/nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\win10\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Download the CMU Pronouncing Dictionary\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Load the CMU Pronouncing Dictionary\n",
    "pronouncing_dict = cmudict.dict()\n",
    "\n",
    "def transcribe_to_arpabet(word):\n",
    "    \"\"\"\n",
    "    Transcribes a word to ARPAbet using the CMU Pronouncing Dictionary.\n",
    "    If the word is not found in the dictionary, return None.\n",
    "    \"\"\"\n",
    "    # Convert the word to lowercase\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Check if the word exists in the CMU Pronouncing Dictionary\n",
    "    if word in pronouncing_dict:\n",
    "        # Return the ARPAbet transcription of the word\n",
    "        return pronouncing_dict[word][0]\n",
    "    else:\n",
    "        # If the word is not found, return None\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Sample sentences\n",
    "    sentences = [\n",
    "    \"Uh huh and then uh what time did you get up this morning?\"\n",
    "    \"Well I guess you could say that uh huh.\"\n",
    "    \"I don't know uh huh.\"\n",
    "    \"An extension cord is something you plug into the wall.\"\n",
    "    \"It's uh it's a long cord that has a plug on one end and then it has three outlets on the other end.\"\n",
    "    \"You can plug like a lamp or a radio or something like that into it.\"\n",
    "    \"So you can have more things plugged in than you normally could have.\"\n",
    "    \"Right.\"\n",
    "    \"And then uh there's also like a surge protector which is kind of like an extension cord but it also protects your appliances from uh power surges.\"\n",
    "    \"So if there's a like a lightning strike or something like that it won't mess up your stuff.\"\n",
    "]\n",
    "\n",
    "    # Extract unique words\n",
    "    unique_words = set()\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        unique_words.update(words)\n",
    "\n",
    "    # Transcribe each unique word to ARPAbet\n",
    "    for word in unique_words:\n",
    "        arpabet_transcription = transcribe_to_arpabet(word)\n",
    "        if arpabet_transcription:\n",
    "            print(f\"{word}: {arpabet_transcription}\")\n",
    "        else:\n",
    "            print(f\"No ARPAbet transcription found for '{word}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc63f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\win10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\cmudict.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Download the CMU Pronouncing Dictionary\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Load the CMU Pronouncing Dictionary\n",
    "pronouncing_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "532a3797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: ['IH1', 'T']\n",
      "up: ['AH1', 'P']\n",
      "what: ['W', 'AH1', 'T']\n",
      "like: ['L', 'AY1', 'K']\n",
      "wo: ['W', 'OW1']\n",
      "the: ['DH', 'AH0']\n",
      "know: ['N', 'OW1']\n",
      "No ARPAbet transcription found for 'it.So'\n",
      "normally: ['N', 'AO1', 'R', 'M', 'AH0', 'L', 'IY0']\n",
      "mess: ['M', 'EH1', 'S']\n",
      "if: ['IH1', 'F']\n",
      "and: ['AH0', 'N', 'D']\n",
      "which: ['W', 'IH1', 'CH']\n",
      "appliances: ['AH0', 'P', 'L', 'AY1', 'AH0', 'N', 'S', 'AH0', 'Z']\n",
      "guess: ['G', 'EH1', 'S']\n",
      "more: ['M', 'AO1', 'R']\n",
      "get: ['G', 'EH1', 'T']\n",
      "cord: ['K', 'AO1', 'R', 'D']\n",
      "long: ['L', 'AO1', 'NG']\n",
      "No ARPAbet transcription found for 'n't'\n",
      "a: ['AH0']\n",
      "could: ['K', 'UH1', 'D']\n",
      "strike: ['S', 'T', 'R', 'AY1', 'K']\n",
      "protector: ['P', 'R', 'AH0', 'T', 'EH1', 'K', 'T', 'ER0']\n",
      "Uh: ['AH1']\n",
      "No ARPAbet transcription found for 'huh.An'\n",
      "No ARPAbet transcription found for 'end.You'\n",
      "than: ['DH', 'AE1', 'N']\n",
      "No ARPAbet transcription found for '?'\n",
      "'s: ['EH1', 'S']\n",
      "there: ['DH', 'EH1', 'R']\n",
      "then: ['DH', 'EH1', 'N']\n",
      "your: ['Y', 'AO1', 'R']\n",
      "plug: ['P', 'L', 'AH1', 'G']\n",
      "lightning: ['L', 'AY1', 'T', 'N', 'IH0', 'NG']\n",
      "or: ['AO1', 'R']\n",
      "power: ['P', 'AW1', 'ER0']\n",
      "one: ['W', 'AH1', 'N']\n",
      "kind: ['K', 'AY1', 'N', 'D']\n",
      "from: ['F', 'R', 'AH1', 'M']\n",
      "No ARPAbet transcription found for 'wall.It'\n",
      "that: ['DH', 'AE1', 'T']\n",
      "plugged: ['P', 'L', 'AH1', 'G', 'D']\n",
      "No ARPAbet transcription found for '.'\n",
      "I: ['AY1']\n",
      "three: ['TH', 'R', 'IY1']\n",
      "radio: ['R', 'EY1', 'D', 'IY0', 'OW2']\n",
      "things: ['TH', 'IH1', 'NG', 'Z']\n",
      "end: ['EH1', 'N', 'D']\n",
      "also: ['AO1', 'L', 'S', 'OW0']\n",
      "have: ['HH', 'AE1', 'V']\n",
      "No ARPAbet transcription found for 'have.Right.And'\n",
      "Well: ['W', 'EH1', 'L']\n",
      "this: ['DH', 'IH1', 'S']\n",
      "other: ['AH1', 'DH', 'ER0']\n",
      "something: ['S', 'AH1', 'M', 'TH', 'IH0', 'NG']\n",
      "do: ['D', 'UW1']\n",
      "on: ['AA1', 'N']\n",
      "time: ['T', 'AY1', 'M']\n",
      "morning: ['M', 'AO1', 'R', 'N', 'IH0', 'NG']\n",
      "but: ['B', 'AH1', 'T']\n",
      "say: ['S', 'EY1']\n",
      "lamp: ['L', 'AE1', 'M', 'P']\n",
      "can: ['K', 'AE1', 'N']\n",
      "stuff: ['S', 'T', 'AH1', 'F']\n",
      "No ARPAbet transcription found for 'huh.I'\n",
      "is: ['IH1', 'Z']\n",
      "an: ['AE1', 'N']\n",
      "into: ['IH0', 'N', 'T', 'UW1']\n",
      "protects: ['P', 'R', 'AH0', 'T', 'EH1', 'K', 'T', 'S']\n",
      "you: ['Y', 'UW1']\n",
      "extension: ['IH0', 'K', 'S', 'T', 'EH1', 'N', 'SH', 'AH0', 'N']\n",
      "No ARPAbet transcription found for 'surges.So'\n",
      "did: ['D', 'IH1', 'D']\n",
      "in: ['IH0', 'N']\n",
      "surge: ['S', 'ER1', 'JH']\n",
      "of: ['AH1', 'V']\n",
      "outlets: ['AW1', 'T', 'L', 'EH2', 'T', 'S']\n",
      "huh: ['HH', 'AH1']\n",
      "uh: ['AH1']\n",
      "has: ['HH', 'AE1', 'Z']\n"
     ]
    }
   ],
   "source": [
    "def transcribe_to_arpabet(word):\n",
    "    \"\"\"\n",
    "    Transcribes a word to ARPAbet using the CMU Pronouncing Dictionary.\n",
    "    If the word is not found in the dictionary, return None.\n",
    "    \"\"\"\n",
    "    # Convert the word to lowercase\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Check if the word exists in the CMU Pronouncing Dictionary\n",
    "    if word in pronouncing_dict:\n",
    "        # Return the ARPAbet transcription of the word\n",
    "        return pronouncing_dict[word][0]\n",
    "    else:\n",
    "        # If the word is not found, return None\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Sample sentences\n",
    "    sentences = [\n",
    "    \"Uh huh and then uh what time did you get up this morning?\"\n",
    "    \"Well I guess you could say that uh huh.\"\n",
    "    \"I don't know uh huh.\"\n",
    "    \"An extension cord is something you plug into the wall.\"\n",
    "    \"It's uh it's a long cord that has a plug on one end and then it has three outlets on the other end.\"\n",
    "    \"You can plug like a lamp or a radio or something like that into it.\"\n",
    "    \"So you can have more things plugged in than you normally could have.\"\n",
    "    \"Right.\"\n",
    "    \"And then uh there's also like a surge protector which is kind of like an extension cord but it also protects your appliances from uh power surges.\"\n",
    "    \"So if there's a like a lightning strike or something like that it won't mess up your stuff.\"\n",
    "]\n",
    "\n",
    "    # Extract unique words\n",
    "    unique_words = set()\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        unique_words.update(words)\n",
    "\n",
    "    # Transcribe each unique word to ARPAbet\n",
    "    for word in unique_words:\n",
    "        arpabet_transcription = transcribe_to_arpabet(word)\n",
    "        if arpabet_transcription:\n",
    "            print(f\"{word}: {arpabet_transcription}\")\n",
    "        else:\n",
    "            print(f\"No ARPAbet transcription found for '{word}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37e71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
